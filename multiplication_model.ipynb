{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governing-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-three",
   "metadata": {},
   "source": [
    "## Creating the dataset\n",
    "$ a * b = y == \\ln(a) + \\ln(b) = \\ln(y) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "grateful-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010229\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_train)\n",
    "mse = mean_squared_error(Y_train, Y_pred)\n",
    "print('MSE: %f' % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-costa",
   "metadata": {},
   "source": [
    "# set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "automotive-certification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size      : (32400, 2)\n",
      "Validation Set Size : (4000, 2)\n",
      "Test Set Size       : (400, 2)\n"
     ]
    }
   ],
   "source": [
    "import random as rnd\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "    \n",
    "for a in range(1, 91): ## train_data\n",
    "    for b in range(1, 91):\n",
    "        X.append( [ np.log( a ) , np.log( b ) ] )\n",
    "        Y.append( [ 1, np.log( a * b ) ] )\n",
    "        \n",
    "        X.append( [ -np.log( a ) , np.log( b ) ] )\n",
    "        Y.append( [ -1, np.log( a * b ) ] )\n",
    "        \n",
    "        X.append( [ np.log( a ) , -np.log( b ) ] )\n",
    "        Y.append( [ -1, np.log( a * b ) ] )\n",
    "        \n",
    "        X.append( [ -np.log( a ) , -np.log( b ) ] )\n",
    "        Y.append( [ 1, np.log( a * b ) ] )\n",
    "    \n",
    "for a in range(81, 91): ## validation_data\n",
    "    for b in range(1, 101):\n",
    "        X_val.append( [ np.log( a ) , np.log( b ) ] )\n",
    "        Y_val.append( [ 1, np.log( a * b ) ] )\n",
    "        \n",
    "        X_val.append( [ -np.log( a ) , np.log( b ) ] )\n",
    "        Y_val.append( [ -1, np.log( a * b ) ] )\n",
    "        \n",
    "        X_val.append( [ np.log( a ) , -np.log( b ) ] )\n",
    "        Y_val.append( [ -1, np.log( a * b ) ] )\n",
    "        \n",
    "        X_val.append( [ -np.log( a ) , -np.log( b ) ] )\n",
    "        Y_val.append( [ 1, np.log( a * b ) ] )\n",
    "        \n",
    "for a in range(91, 101): ## test_data\n",
    "    for b in range(91, 101):\n",
    "        X_test.append( [ np.log( a ) , np.log( b )] )\n",
    "        Y_test.append( [ 1, np.log( a * b ) ] )\n",
    "        \n",
    "        X_test.append( [ -np.log( a ) , np.log( b )] )\n",
    "        Y_test.append( [ -1, np.log( a * b ) ] )\n",
    "        \n",
    "        X_test.append( [ np.log( a ) , -np.log( b )] )\n",
    "        Y_test.append( [ -1, np.log( a * b ) ] )\n",
    "        \n",
    "        X_test.append( [ -np.log( a ) , -np.log( b )] )\n",
    "        Y_test.append( [ 1, np.log( a * b ) ] )\n",
    "        \n",
    "X_train = np.array( X )        \n",
    "Y_train = np.array( Y )\n",
    "\n",
    "X_val = np.array( X_val )        \n",
    "Y_val = np.array( Y_val )\n",
    "\n",
    "X_test = np.array( X_test )        \n",
    "Y_test = np.array( Y_test )\n",
    "\n",
    "print( f\"Train Set Size      : {X_train.shape}\" )\n",
    "print( f\"Validation Set Size : {X_val.shape}\"   )\n",
    "print( f\"Test Set Size       : {X_test.shape}\"   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "inner-playback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 50,702\n",
      "Trainable params: 50,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(layers.Dense(50, input_dim=2, activation=tf.nn.relu))\n",
    "\n",
    "model.add(layers.Dense(100, activation=tf.nn.relu))\n",
    "\n",
    "model.add(layers.Dense(200, activation=tf.nn.relu))\n",
    "\n",
    "model.add(layers.Dense(100, activation=tf.nn.relu))\n",
    "\n",
    "model.add(layers.Dense(50, activation=tf.nn.relu))\n",
    "\n",
    "model.add(layers.Dense(2, activation='linear'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "angry-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-holocaust",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "damaged-stage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size      : (40000, 2)\n",
      "Validation Set Size : (4000, 2)\n",
      "Test Set Size       : (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "import random as rnd\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for i in range( 10000 ): ## train data random\n",
    "    r1 = rnd.randint(1, 90)\n",
    "    r2 = rnd.randint(1, 100)\n",
    "    X_train.append( [   np.log( r1 ) , np.log( r2 ) ] )\n",
    "    Y_train.append( [1, np.log( r1 ) + np.log( r2 ) ] )\n",
    "    \n",
    "    r1 = rnd.randint(1, 90)\n",
    "    r2 = rnd.randint(1, 100)\n",
    "    X_train.append( [   -np.log( r1 ) , np.log( r2 ) ] )\n",
    "    Y_train.append( [-1, np.log( r1 ) + np.log( r2 ) ] )\n",
    "    \n",
    "    r1 = rnd.randint(1, 90)\n",
    "    r2 = rnd.randint(1, 100)\n",
    "    X_train.append( [    np.log( r1 ) , -np.log( r2 ) ] )\n",
    "    Y_train.append( [-1, np.log( r1 ) +  np.log( r2 ) ] )\n",
    "    \n",
    "    r1 = rnd.randint(1, 90)\n",
    "    r2 = rnd.randint(1, 100)\n",
    "    X_train.append( [   -np.log( r1 ) , -np.log( r2 ) ] )\n",
    "    Y_train.append( [1,  np.log( r1 ) +  np.log( r2 ) ] )\n",
    "    \n",
    "for a in range(81, 91): ## validation_data\n",
    "    for b in range(1, 101):\n",
    "        X_val.append( [   np.log( a ) , np.log( b ) ] )\n",
    "        Y_val.append( [1, np.log( a ) + np.log( b ) ] )\n",
    "        \n",
    "        X_val.append( [   -np.log( a ) , np.log( b ) ] )\n",
    "        Y_val.append( [-1, np.log( a ) + np.log( b ) ] )\n",
    "        \n",
    "        X_val.append( [    np.log( a ) , -np.log( b ) ] )\n",
    "        Y_val.append( [-1, np.log( a ) +  np.log( b ) ] )\n",
    "        \n",
    "        X_val.append( [  -np.log( a ) , -np.log( b ) ] )\n",
    "        Y_val.append( [1, np.log( a ) +  np.log( b ) ] )\n",
    "        \n",
    "for a in range(91, 101): ## test_data\n",
    "    for b in range(1, 101):\n",
    "        X_test.append( [   np.log( a ) , np.log( b ) ] )\n",
    "        Y_test.append( [1, np.log( a ) + np.log( b ) ] )\n",
    "        \n",
    "        X_test.append( [   -np.log( a ) , np.log( b ) ] )\n",
    "        Y_test.append( [-1, np.log( a ) + np.log( b ) ] )\n",
    "        \n",
    "        X_test.append( [    np.log( a ) , -np.log( b ) ] )\n",
    "        Y_test.append( [-1, np.log( a ) +  np.log( b ) ] )\n",
    "        \n",
    "        X_test.append( [  -np.log( a ) , -np.log( b ) ] )\n",
    "        Y_test.append( [1, np.log( a ) +  np.log( b ) ] )\n",
    "        \n",
    "X_train = np.array( X_train )        \n",
    "Y_train = np.array( Y_train )\n",
    "\n",
    "X_val = np.array( X_val )        \n",
    "Y_val = np.array( Y_val )\n",
    "\n",
    "X_test = np.array( X_test )        \n",
    "Y_test = np.array( Y_test )\n",
    "\n",
    "print( f\"Train Set Size      : {X_train.shape}\" )\n",
    "print( f\"Validation Set Size : {X_val.shape}\"   )\n",
    "print( f\"Test Set Size       : {X_test.shape}\"   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-update",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "limited-development",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,802\n",
      "Trainable params: 2,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(50, input_dim=2, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(50, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(2 , activation='linear'))\n",
    "\n",
    "model.summary()\n",
    "model.compile( optimizer='adam', loss='MSE' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-samoa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "administrative-falls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1125/1125 [==============================] - 1s 356us/step - loss: 2.0298 - val_loss: 0.0261\n",
      "Epoch 2/200\n",
      "1125/1125 [==============================] - 0s 313us/step - loss: 0.0233 - val_loss: 0.0168\n",
      "Epoch 3/200\n",
      "1125/1125 [==============================] - 0s 314us/step - loss: 0.0168 - val_loss: 0.0141\n",
      "Epoch 4/200\n",
      "1125/1125 [==============================] - 0s 311us/step - loss: 0.0134 - val_loss: 0.0114\n",
      "Epoch 5/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0119 - val_loss: 0.0128\n",
      "Epoch 6/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 7/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 8/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 9/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 10/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 11/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 12/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 13/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0111 - val_loss: 0.0119\n",
      "Epoch 14/200\n",
      "1125/1125 [==============================] - 0s 306us/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 15/200\n",
      "1125/1125 [==============================] - 0s 314us/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 16/200\n",
      "1125/1125 [==============================] - 0s 329us/step - loss: 0.0116 - val_loss: 0.0102\n",
      "Epoch 17/200\n",
      "1125/1125 [==============================] - 0s 322us/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 18/200\n",
      "1125/1125 [==============================] - 0s 322us/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 19/200\n",
      "1125/1125 [==============================] - 0s 313us/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 20/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 21/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0121 - val_loss: 0.0108\n",
      "Epoch 22/200\n",
      "1125/1125 [==============================] - 0s 311us/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 23/200\n",
      "1125/1125 [==============================] - 0s 314us/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 24/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 25/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 26/200\n",
      "1125/1125 [==============================] - 0s 306us/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 27/200\n",
      "1125/1125 [==============================] - 0s 312us/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 28/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0117 - val_loss: 0.0109\n",
      "Epoch 29/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 30/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0104 - val_loss: 0.0117\n",
      "Epoch 31/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 32/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 33/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 34/200\n",
      "1125/1125 [==============================] - 0s 313us/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 35/200\n",
      "1125/1125 [==============================] - 0s 320us/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 36/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 37/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 38/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 39/200\n",
      "1125/1125 [==============================] - 0s 322us/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 40/200\n",
      "1125/1125 [==============================] - 0s 311us/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 41/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 42/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 43/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 44/200\n",
      "1125/1125 [==============================] - 0s 316us/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 45/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 46/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0113 - val_loss: 0.0097\n",
      "Epoch 47/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 48/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 49/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 50/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 51/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 52/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 53/200\n",
      "1125/1125 [==============================] - 0s 312us/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 54/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 55/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 56/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 57/200\n",
      "1125/1125 [==============================] - 0s 312us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 58/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0111 - val_loss: 0.0098\n",
      "Epoch 59/200\n",
      "1125/1125 [==============================] - 0s 311us/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 60/200\n",
      "1125/1125 [==============================] - 0s 326us/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 61/200\n",
      "1125/1125 [==============================] - 0s 329us/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 62/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0115 - val_loss: 0.0098\n",
      "Epoch 63/200\n",
      "1125/1125 [==============================] - 0s 339us/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 64/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 65/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1125/1125 [==============================] - 0s 329us/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 67/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 68/200\n",
      "1125/1125 [==============================] - 0s 329us/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 69/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 70/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0098 - val_loss: 0.0101\n",
      "Epoch 71/200\n",
      "1125/1125 [==============================] - 0s 315us/step - loss: 0.0096 - val_loss: 0.0107\n",
      "Epoch 72/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 73/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0097 - val_loss: 0.0099\n",
      "Epoch 74/200\n",
      "1125/1125 [==============================] - 0s 318us/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 75/200\n",
      "1125/1125 [==============================] - 0s 312us/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 76/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 77/200\n",
      "1125/1125 [==============================] - 0s 314us/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 331us/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 79/200\n",
      "1125/1125 [==============================] - 0s 325us/step - loss: 0.0097 - val_loss: 0.0099\n",
      "Epoch 80/200\n",
      "1125/1125 [==============================] - 0s 323us/step - loss: 0.0098 - val_loss: 0.0109\n",
      "Epoch 81/200\n",
      "1125/1125 [==============================] - 0s 315us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 82/200\n",
      "1125/1125 [==============================] - 0s 326us/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 83/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 84/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 85/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 86/200\n",
      "1125/1125 [==============================] - 0s 301us/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 87/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 88/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 89/200\n",
      "1125/1125 [==============================] - 0s 300us/step - loss: 0.0111 - val_loss: 0.0099\n",
      "Epoch 90/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 91/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 92/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 93/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 94/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 95/200\n",
      "1125/1125 [==============================] - 0s 333us/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 96/200\n",
      "1125/1125 [==============================] - 0s 336us/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 97/200\n",
      "1125/1125 [==============================] - 0s 321us/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 98/200\n",
      "1125/1125 [==============================] - 0s 313us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 99/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 100/200\n",
      "1125/1125 [==============================] - 0s 321us/step - loss: 0.0096 - val_loss: 0.0106\n",
      "Epoch 101/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 102/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 103/200\n",
      "1125/1125 [==============================] - 0s 306us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 104/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 105/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 106/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 107/200\n",
      "1125/1125 [==============================] - 0s 329us/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 108/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 109/200\n",
      "1125/1125 [==============================] - 0s 334us/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 110/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 111/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 112/200\n",
      "1125/1125 [==============================] - 0s 306us/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 113/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0097 - val_loss: 0.0103\n",
      "Epoch 114/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 115/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0102 - val_loss: 0.0098\n",
      "Epoch 116/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 117/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 118/200\n",
      "1125/1125 [==============================] - 0s 306us/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 119/200\n",
      "1125/1125 [==============================] - 0s 306us/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 120/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 121/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 122/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 123/200\n",
      "1125/1125 [==============================] - 0s 315us/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 124/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 125/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 126/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 127/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 128/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 129/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 130/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 131/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 132/200\n",
      "1125/1125 [==============================] - 0s 311us/step - loss: 0.0103 - val_loss: 0.0095\n",
      "Epoch 133/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0097 - val_loss: 0.0098\n",
      "Epoch 134/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0106 - val_loss: 0.0096\n",
      "Epoch 135/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 136/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 137/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 138/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 139/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0096 - val_loss: 0.0095\n",
      "Epoch 140/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 141/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 142/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 143/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 144/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 145/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 146/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 147/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 148/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 149/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 150/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 151/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 152/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 153/200\n",
      "1125/1125 [==============================] - 0s 308us/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 154/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0106 - val_loss: 0.0095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0098 - val_loss: 0.0100\n",
      "Epoch 156/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0097 - val_loss: 0.0098\n",
      "Epoch 157/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 158/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 159/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 160/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 161/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 162/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 163/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 164/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 165/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 166/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 167/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 168/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 169/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0106 - val_loss: 0.0096\n",
      "Epoch 170/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0096 - val_loss: 0.0100\n",
      "Epoch 171/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 172/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 173/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 174/200\n",
      "1125/1125 [==============================] - 0s 302us/step - loss: 0.0115 - val_loss: 0.0099\n",
      "Epoch 175/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 176/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0100 - val_loss: 0.0097\n",
      "Epoch 177/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 178/200\n",
      "1125/1125 [==============================] - 0s 303us/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 179/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0095 - val_loss: 0.0097\n",
      "Epoch 180/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 181/200\n",
      "1125/1125 [==============================] - 0s 304us/step - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 182/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 183/200\n",
      "1125/1125 [==============================] - 0s 305us/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 184/200\n",
      "1125/1125 [==============================] - 0s 313us/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 185/200\n",
      "1125/1125 [==============================] - 0s 313us/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 186/200\n",
      "1125/1125 [==============================] - 0s 320us/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 187/200\n",
      "1125/1125 [==============================] - 0s 326us/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 188/200\n",
      "1125/1125 [==============================] - 0s 326us/step - loss: 0.0101 - val_loss: 0.0095\n",
      "Epoch 189/200\n",
      "1125/1125 [==============================] - 0s 319us/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 190/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 191/200\n",
      "1125/1125 [==============================] - 0s 321us/step - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 192/200\n",
      "1125/1125 [==============================] - 0s 310us/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 193/200\n",
      "1125/1125 [==============================] - 0s 311us/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 194/200\n",
      "1125/1125 [==============================] - 0s 309us/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 195/200\n",
      "1125/1125 [==============================] - 0s 317us/step - loss: 0.0100 - val_loss: 0.0097\n",
      "Epoch 196/200\n",
      "1125/1125 [==============================] - 0s 333us/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 197/200\n",
      "1125/1125 [==============================] - 0s 324us/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 198/200\n",
      "1125/1125 [==============================] - 0s 307us/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 199/200\n",
      "1125/1125 [==============================] - 0s 314us/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 200/200\n",
      "1125/1125 [==============================] - 0s 324us/step - loss: 0.0102 - val_loss: 0.0103\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( X_train, Y_train, validation_split=0.1, \n",
    "                    epochs=200 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-wheel",
   "metadata": {},
   "source": [
    "## Testing and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "impaired-bikini",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.ylim([0, 0.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fiscal-prefix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0061\n"
     ]
    }
   ],
   "source": [
    "loss_test = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print( 'Test loss: %.4f' % loss_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "horizontal-analyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010549\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_train)\n",
    "mse = mean_squared_error(Y_train, Y_pred)\n",
    "print('MSE: %f' % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "signed-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_ML(num1, num2):\n",
    "    \n",
    "    num1_abs = np.abs(num1)\n",
    "    num2_abs = np.abs(num2)\n",
    "    \n",
    "    if np.abs( num1 ) != num1 and np.abs( num2 ) == num2:\n",
    "        test = np.array( [ -np.log( num1_abs ), np.log( num2_abs ) ] ).reshape( -1, 1 ).T\n",
    "    elif np.abs( num1 ) == num1 and np.abs( num2 ) != num2:\n",
    "        test = np.array( [ np.log( num1_abs ), -np.log( num2_abs ) ] ).reshape( -1, 1 ).T\n",
    "    elif np.abs( num1 ) != num1 and np.abs( num2 ) != num2:\n",
    "        test = np.array( [ -np.log( num1_abs ), -np.log( num2_abs ) ] ).reshape( -1, 1 ).T\n",
    "    else:\n",
    "        test = np.array( [ np.log( num1_abs ), np.log( num2_abs ) ] ).reshape( -1, 1 ).T\n",
    "        \n",
    "    [(sign, pred)] = model.predict( test )\n",
    "    \n",
    "    if sign > 0:\n",
    "        pred =  1 * np.exp( pred )\n",
    "    else:\n",
    "        pred = -1 * np.exp( pred )\n",
    "\n",
    "    return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "medieval-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10    x  2     => predicted : 19.90590476989746    \treal : 20\n",
      "  3.5   x  4     => predicted : 14.045559883117676   \treal : 14.0\n",
      " -5     x  4     => predicted : -19.842121124267578    \treal : -20\n",
      "  10.5  x  -1.5   => predicted : -13.412880897521973   \treal : -15.75\n",
      "  200   x  200   => predicted : 39341.19921875     \treal : 40000\n",
      "  -1.5  x  48.6  => predicted : -69.90042877197266    \treal : -72.9\n",
      "  1.5   x  48.6  => predicted : 70.77992248535156   \treal : 72.9\n",
      "  36    x  34    => predicted : 1214.988525390625     \treal : 1224\n"
     ]
    }
   ],
   "source": [
    "print( f\"  10    x  2     => predicted : {multiply_ML(10, 2)}    \\treal : {10 * 2}\" )\n",
    "print( f\"  3.5   x  4     => predicted : {multiply_ML(3.5, 4)}   \\treal : {3.5 * 4}\" )\n",
    "print( f\" -5     x  4     => predicted : {multiply_ML(-5, 4)}    \\treal : {-5 * 4}\" )\n",
    "print( f\"  10.5  x  -1.5   => predicted : {multiply_ML(10.5, -1.5)}   \\treal : {10.5 * -1.5}\" ) \n",
    "print( f\"  200   x  200   => predicted : {multiply_ML(200, 200)}     \\treal : {200 * 200}\" ) \n",
    "print( f\"  -1.5  x  48.6  => predicted : {multiply_ML(-1.5, 48.6)}    \\treal : {-1.5 * 48.6}\" ) \n",
    "print( f\"  1.5   x  48.6  => predicted : {multiply_ML(1.5, 48.6)}   \\treal : {1.5 * 48.6}\" ) \n",
    "print( f\"  36    x  34    => predicted : {multiply_ML(36, 34)}     \\treal : {36 * 34}\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "distinguished-internet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21 x 0.5 => predicted : -42.01084899902344   real : 10.5\n"
     ]
    }
   ],
   "source": [
    "## Model fails when one of the numbers is between 0 and 1\n",
    "print( f\" 21 x 0.5 => predicted : {multiply_ML(21, 0.5)}   real : {21 * 0.5}\" ) ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cooked-realtor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.171882629394531"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply_ML(2, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-programmer",
   "metadata": {},
   "source": [
    "### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "stopped-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "    np.save(f, Y_train)\n",
    "    \n",
    "with open('validation.npy', 'wb') as f:\n",
    "    np.save(f, X_val)\n",
    "    np.save(f, Y_val)\n",
    "    \n",
    "with open('test.npy', 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "    np.save(f, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-birmingham",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "rural-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-bradley",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.npy', 'rb') as f:\n",
    "    X_train = np.load(f)\n",
    "    Y_train = np.load(f)\n",
    "    \n",
    "with open('validation.npy', 'rb') as f:\n",
    "    X_val = np.load(f)\n",
    "    Y_val = np.load(f)\n",
    "    \n",
    "with open('test.npy', 'rb') as f:\n",
    "    X_test = np.load(f)\n",
    "    Y_test = np.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('tensor': conda)",
   "language": "python",
   "name": "python38664bittensorconda609c154a24774adb9b1b7af81d36243e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
